# End-to-End Global Food Prices Inflation Pipeline

![AWS](https://img.shields.io/badge/AWS-232F3E?style=for-the-badge&logo=amazon-aws&logoColor=white)
![AWS Glue](https://img.shields.io/badge/AWS_Glue-3E4B59?style=for-the-badge&logo=aws-glue&logoColor=white)
![AWS Lambda](https://img.shields.io/badge/AWS_Lambda-FF9900?style=for-the-badge&logo=aws-lambda&logoColor=white)
![Databricks](https://img.shields.io/badge/Databricks-FF3621?style=for-the-badge&logo=databricks&logoColor=white)
![Apache Spark](https://img.shields.io/badge/Apache_Spark-E25A1C?style=for-the-badge&logo=apache-spark&logoColor=white)

This project demonstrates a complete, event-driven data engineering pipeline built on a dual-platform architecture (AWS and Databricks). It ingests, cleans, and analyzes over **3.1 million records** of global food price data to calculate reliable inflation metrics.

The pipeline follows a **Medallion Architecture** (Raw, Silver, Gold layers) and showcases advanced data quality standardization, orchestration, and analytics.

---

## ðŸ›ï¸ Project Architecture

The entire workflow is automated and event-driven:

1.  **Raw Layer (Ingestion):** The raw CSV data file is uploaded to an S3 bucket (`/raw` prefix).
2.  **Orchestration (Trigger):** An **AWS Lambda** function, triggered by the S3 upload event, starts the pipeline by invoking the AWS Glue Crawler.
3.  **ETL (Silver Layer):**
    * The **AWS Glue Crawler** scans the raw data to update the schema in the Glue Data Catalog.
    * The Lambda function (after the crawler completes) triggers the main **AWS Glue ETL Job**.
    * The Glue job reads the raw data, performs complex data standardization, and writes the clean, partitioned Parquet data to the S3 Silver layer (`/silver` prefix).
4.  **Analytics (Gold Layer):**
    * A **Databricks** notebook connects directly to the S3 Silver layer to read the partitioned Parquet files.
    * **Spark SQL** is used to perform advanced transformations, including Window Functions (LAG) and outlier filtering.
    * The final aggregated metrics are saved as **Delta Tables** (Gold layer) within Databricks.
5.  **Reporting (Dashboard):**
    * A **Databricks Dashboard** is built directly on top of the final Gold Delta Tables for real-time visualization of inflation metrics.

---

## ðŸŽ¯ Key Technical Features

* **Dual-Platform Design:** Demonstrates proficiency in both **AWS Glue** for serverless, scalable ETL and **Databricks** for high-performance Spark analytics.
* **Event-Driven Orchestration:** Uses **AWS Lambda** (`boto3`) to create a fully automated, asynchronous pipeline that waits for the crawler to finish before starting the ETL job.
* **Advanced Data Quality & Standardization:**
    * Successfully processed and standardized **~600,000 inconsistent unit records** (e.g., 'KG', 'G', '100 KG', '5L', '100ML', etc.) into two clean columns (`price_per_kg` and `price_per_litre`) using complex SQL `CASE` statements and `REGEXP_REPLACE`.
    * Engineered a new `commodity_category` dimension (e.g., 'Rice', 'Maize/Corn') from broad source categories (e.g., 'cereals and tubers') to enable granular reporting.
* **Advanced Analytics & Outlier Filtering:**
    * Calculated **Quarter-over-Quarter (QoQ) Inflation** using Spark SQL **Window Functions (LAG)**.
    * Identified and **filtered extreme price outliers** (e.g., a **>21,000%** quarterly spike in 'Sugar' prices) to create statistically reliable and accurate final metrics.
* **Optimized Data Lake:** Uses **Parquet** for the Silver layer (partitioned by `country` and `market`) and **Delta Lake** for the Gold layer (managed tables) for maximum query performance.

---

## ðŸ“Š Data Source

This project uses the **Global Food Prices Dataset** from the World Food Programme, which contains over 3.1 million rows of price data.

* **Source:** [Kaggle - Global Food Prices Dataset](https://www.kaggle.com/datasets/adrianjuliusaluoch/global-food-prices)

---

## ðŸ“‚ Repository Contents

* `lambda_orchestrator/`
    * `lambda_function.py`: The Python (Boto3) script for the Lambda function that orchestrates the Glue Crawler and Glue Job.
* `glue_etl_job/`
    * `glue_studio_visual.png`: A screenshot of the complete visual ETL flow designed in AWS Glue Studio.
    * `glue_generated_script.py`: The full PySpark script automatically generated by AWS Glue Studio from the visual job.
* `databricks_analytics/`
    * `databricks_gold_layer_inflation.ipynb`: The Databricks notebook (Python/Spark SQL) used to read from the S3 Silver layer, perform the inflation calculations (QoQ, Annualized), filter outliers, and create the final Gold Delta Tables.
    * `databricks_dashboard.png`: A screenshot of the final Databricks dashboard visualizing the country-wise and commodity-wise inflation rates.

---

## ðŸš€ How to Replicate

1.  **AWS Setup:**
    * Create an S3 bucket with `raw/`, `silver/`, and `gold/` folders.
    * Create two IAM Roles (one for Glue, one for Lambda) with the necessary `S3`, `Glue`, and `iam:PassRole` permissions.
    * In AWS Glue, create the `food-prices-raw-crawler`.
    * In AWS Glue Studio, build the visual ETL job (see `glue_studio_visual.png`) and name it (e.g., `Si`).
    * Deploy the Lambda function using `lambda_function.py` and set the S3 "Object Created" trigger on the `raw/` folder.
2.  **Databricks Setup:**
    * Set up a Databricks workspace (Community Edition or standard).
    * Ensure your cluster has the necessary connector/credentials to read from your S3 bucket.
3.  **Run Pipeline:**
    * Upload the raw CSV data to the `s3://.../raw/` folder. The Lambda trigger will automatically start the Glue pipeline.
    * Once the Glue job completes, the Silver layer (`s3://.../silver/`) will be populated with partitioned Parquet files.
4.  **Generate Analytics:**
    * Open the `databricks_gold_layer_inflation.ipynb` notebook.
    * Run the notebook cells to read the Silver data, perform the Gold layer calculations, and save the final `gold_...` Delta Tables.
    * Use the Databricks SQL Dashboard feature to visualize the final tables.
